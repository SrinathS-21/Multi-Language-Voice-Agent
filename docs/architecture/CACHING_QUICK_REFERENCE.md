# Caching Quick Reference

**ğŸ“„ Full Report:** See [CACHING_ARCHITECTURE.md](./CACHING_ARCHITECTURE.md)

---

## ğŸ¯ TL;DR for Production

**Current Setup:**
- âœ… Works great locally (1 worker)
- ğŸŸ¡ Needs tuning for production (10+ workers)

**Main Issue:**
Caches are **per-process**, not **shared across workers**
â†’ Cache hit rate drops from 90% to 20% with round-robin load balancing

**Quick Fix:**
```nginx
# nginx.conf - Use sticky sessions
upstream livekit_agents {
  ip_hash;  # Same user â†’ same worker
  server worker1:3000;
  server worker2:3000;
}
```
â†’ Hit rate improves to 60-70%

**Better Fix (Week 1):**
Implement Redis shared cache
â†’ Hit rate: 85-95%

---

## ğŸ“Š Cache Inventory

| What Gets Cached | Where | TTL | Shared? | Prod Ready? |
|------------------|-------|-----|---------|-------------|
| **TTS Audio** (phrases) | Memory Map | 1h | âŒ Per agent | ğŸŸ¡ OK |
| **RAG Results** (knowledge search) | LRU Singleton | 5m | âŒ Per worker | ğŸŸ¡ Needs Redis |
| **Agent Config** | Memory Map | 1m | âŒ Per worker | ğŸŸ¢ Good |
| **Agent Prompts** | Memory Map | 10m | âŒ Per worker | ğŸŸ¢ Good |
| **WebSocket Connections** | ConnectionPool | 1h | âŒ Per agent | ğŸŸ¢ Good |

---

## ğŸ” How Each Cache Works

### 1. TTS Phrase Cache
```
User says: "Hello"
Agent responds: "Hi, how can I help?"

First time:
  Greeting â†’ Sarvam API â†’ 1000ms â†’ Cached

Second time:
  Greeting â†’ Cache hit â†’ 5ms âš¡
```

**Isolation:** Each agent has own cache
- Agent A greeting â‰  Agent B greeting âœ…
- User changes greeting â†’ New cache entry âœ…

### 2. RAG Result Cache
```
User asks: "What is knee pain?"
Agent searches knowledge base

First time:
  Query â†’ Convex â†’ Vector search â†’ 200ms â†’ Cached

Same question (within 5 min):
  Query â†’ Cache hit â†’ 1-2ms âš¡
```

**Problem in Production:**
```
Worker 1: "knee pain" â†’ Cached âœ…
Worker 2: "knee pain" â†’ Cache MISS âŒ (duplicate query!)
Worker 3: "knee pain" â†’ Cache MISS âŒ (duplicate query!)
```

### 3. Agent Config Cache
```
Agent starts â†’ Load config from database

First time:
  Agent ID â†’ Convex query â†’ 100ms â†’ Cached (1 min)

Within 1 minute:
  Agent ID â†’ Cache hit â†’ 1ms âš¡
```

**TTL = 1 minute** (short to pick up config changes quickly)

### 4. Agent Prompt Cache
```
Agent starts â†’ Load full system prompt

First time:
  Agent ID â†’ Convex query â†’ 100ms â†’ Cached (10 min)

Within 10 minutes:
  Agent ID â†’ Cache hit â†’ 1-2ms âš¡

Admin updates prompt:
  New timestamp â†’ NEW cache key â†’ Fresh fetch âœ…
```

**Smart:** Uses timestamp-based keys for auto-invalidation

### 5. WebSocket Connection Pool
```
Agent starts â†’ Pre-warm 2 connections to Sarvam API

TTS Request 1: Use WS1 (0ms connection time)
TTS Request 2: Use WS2 (0ms connection time)
TTS Request 3: Reuse WS1 (0ms connection time)

vs No pooling:
  Each request: 50-200ms connection overhead âŒ
```

---

## ğŸš¨ Production Concerns

### Memory Usage
```
Local (1 worker):
  TTS cache: 5MB
  RAG cache: 2.5MB
  Config cache: 1MB
  Total: ~10MB âœ…

Production (10 workers):
  TTS cache: 5MB Ã— 10 = 50MB
  RAG cache: 2.5MB Ã— 10 = 25MB
  Config cache: 1MB Ã— 10 = 10MB
  Total: ~100MB âœ… (acceptable)
```

### Cache Hit Rates

**Local (1 worker):**
```
TTS Phrase: 90% hit rate âœ…
RAG Result: 80% hit rate âœ…
Agent Config: 95% hit rate âœ…
```

**Production (10 workers, round-robin):**
```
TTS Phrase: 20% hit rate âŒ
RAG Result: 15% hit rate âŒ (lots of duplicate queries!)
Agent Config: 85% hit rate âš ï¸
```

### Cost Impact

**Without shared cache:**
```
1000 users ask "knee pain"
â†’ 1000 Convex queries
â†’ Cost: $X
```

**With Redis shared cache:**
```
1000 users ask "knee pain"
â†’ 1 Convex query (first user)
â†’ 999 cache hits
â†’ Cost: $X / 1000 âš¡
```

---

## âœ… Production Deployment Checklist

### Before Launch (P0)
- [ ] Configure sticky sessions (nginx/ALB)
- [ ] Add cache size limits
- [ ] Set up monitoring
- [ ] Test with 100+ concurrent users

### Week 1 (P1)
- [ ] Implement Redis for RAG cache
- [ ] Add cache metrics dashboard
- [ ] Tune TTL values based on metrics

### Month 1 (P2)
- [ ] Migrate all caches to Redis
- [ ] Add cache warming on startup
- [ ] Implement cache invalidation webhooks

---

## ğŸ”§ Configuration Tuning

### For Low Traffic (<100 concurrent users)
```typescript
// Current settings are fine
PHRASE_CACHE_MAX_SIZE = 100
RAG_CACHE_SIZE = 500
```

### For High Traffic (>1000 concurrent users)
```typescript
// Increase cache sizes
PHRASE_CACHE_MAX_SIZE = 500
RAG_CACHE_SIZE = 2000

// Longer TTL (reduce DB load)
AGENT_CONFIG_TTL = 300  // 5 minutes (was 1 min)
RAG_RESULT_TTL = 1800   // 30 minutes (was 5 min)

// More pre-warmed connections
tts.prewarm(5);  // Instead of 2
```

---

## ğŸ“ When to Invalidate Caches

### Manually Invalidate When:
1. **Knowledge base updated**
   ```typescript
   await VoiceKnowledgeService.invalidateCache(orgId);
   ```

2. **Agent config changed**
   - Auto-invalidates (1 min TTL) âœ…

3. **Agent prompt updated**
   - Auto-invalidates (timestamp keys) âœ…

4. **System upgrade/restart**
   - All caches cleared automatically âœ…

---

## ğŸ“ Key Takeaways

1. **Caches are isolated per worker** (not shared)
2. **Use sticky sessions** for better hit rates
3. **Implement Redis** for production scale
4. **Monitor cache metrics** to tune settings
5. **TTL is your friend** (auto-cleanup)

---

**Need Help?**
- Full details: [CACHING_ARCHITECTURE.md](./CACHING_ARCHITECTURE.md)
- Code locations: Search for `cache` in `src/` directory
