# Caching Architecture - Detailed Report

**Date:** January 19, 2026  
**System:** LiveKit Voice Agent with Sarvam AI Integration  
**Purpose:** Multi-layer caching for performance optimization

---

## üìä Executive Summary

This document provides a comprehensive analysis of all caching mechanisms in the voice agent system, their behavior in local vs production environments, and recommendations for production deployment.

### Key Findings
- **5 Independent Cache Layers** (no conflicts between them)
- **4 Thread-Safe** (Map-based caches in Node.js single-threaded model)
- **1 Process-Isolated** (TTS Phrase Cache per agent instance)
- **Production Risk Level:** üü° Medium (requires proper configuration)

---

## üèóÔ∏è Cache Layers Overview

| Layer | Type | Scope | TTL | Concurrency Model | Production Risk |
|-------|------|-------|-----|-------------------|-----------------|
| **1. TTS Phrase Cache** | In-Memory Map | Per TTS Instance | 1 hour | Process-isolated | üü¢ Low |
| **2. RAG Result Cache** | LRU Cache (Singleton) | Global Shared | 5 minutes | Thread-safe | üü° Medium |
| **3. Agent Config Cache** | In-Memory Map | Per Service Instance | 1 minute | Thread-safe | üü¢ Low |
| **4. Agent Prompt Cache** | In-Memory Map | Per Service Instance | 10 minutes | Thread-safe | üü¢ Low |
| **5. WebSocket Connection Pool** | ConnectionPool | Per TTS Instance | 1 hour | Thread-safe | üü¢ Low |

---

## üîç Detailed Cache Analysis

### 1. TTS Phrase Cache

**Location:** `src/plugins/sarvam_tts.ts`

**Purpose:** Cache synthesized audio frames for frequently used phrases to avoid repeated Sarvam API calls.

#### Implementation Details
```typescript
// Instance property (isolated per agent)
private phraseCache: Map<string, { frames: AudioFrame[]; createdAt: number }> = new Map();
private static PHRASE_CACHE_MAX_SIZE = 100;
private static PHRASE_CACHE_TTL_MS = 3600_000; // 1 hour

// Cache key includes voice settings for uniqueness
_getPhraseCacheKey(text: string): string {
  const configString = `${text}|${this.speaker}|${this.languageCode}|${this.pace}|${this.pitch}|${this.model}`;
  return createHash('md5').update(configString).digest('hex');
}
```

#### Lifecycle
- **Created:** Once per TTS plugin instance (one per voice session)
- **Populated:** 
  - Pre-warmed on agent startup with agent-specific greeting
  - Auto-populated for phrases <200 chars on first use
- **Invalidated:** 
  - TTL-based expiration (1 hour)
  - FIFO eviction when size > 100

#### Local vs Production Behavior

**Local (Single Developer):**
```
Agent A starts ‚Üí TTS Instance 1 ‚Üí Cache A (empty)
‚îî‚îÄ Greeting synthesized ‚Üí Cached in Cache A
‚îî‚îÄ Greeting repeated ‚Üí Cache hit (~5ms)

Agent B starts ‚Üí TTS Instance 2 ‚Üí Cache B (empty)
‚îî‚îÄ Different greeting ‚Üí Cached in Cache B
```

**Production (100 Concurrent Users):**
```
User 1 ‚Üí Agent A Instance 1 ‚Üí Cache 1 (greeting A cached)
User 2 ‚Üí Agent A Instance 2 ‚Üí Cache 2 (greeting A cached)
User 3 ‚Üí Agent B Instance 1 ‚Üí Cache 3 (greeting B cached)
...
User 100 ‚Üí Agent X Instance N ‚Üí Cache N (greeting X cached)

Total Memory: ~100 TTS instances √ó 100 cached phrases √ó ~50KB/phrase = ~500MB
```

#### Concurrency Characteristics

**‚úÖ Thread-Safe:** YES
- Node.js single-threaded event loop
- No race conditions within single process
- JavaScript `Map` operations are atomic

**‚ùå Process-Shared:** NO
- Each worker process has isolated cache
- Same agent on different workers = duplicate caches
- Cache warming happens per process

**‚ö†Ô∏è Production Concerns:**

1. **Memory Usage**
   - Worst case: 100 agents √ó 5 workers = 500 cache instances
   - Each cache: ~5MB (100 phrases √ó 50KB avg)
   - **Total: ~2.5GB** across cluster

2. **Cache Warming Overhead**
   - Each worker pre-warms greetings independently
   - First request per worker: ~1000ms (no cache yet)
   - Solution: Use sticky sessions or shared Redis

3. **No Cross-Process Sharing**
   - User 1 on Worker 1: greeting cached
   - User 2 (same agent) on Worker 2: cache MISS
   - **Hit Rate: ~20%** in round-robin load balancing

#### Production Recommendations

**Option A: Accept Current Design (Simple)**
- ‚úÖ Zero external dependencies
- ‚úÖ Fast cache hits (5ms)
- ‚ùå Duplicate cache warming (~5-10 seconds per worker)
- ‚ùå Lower hit rate with multiple workers
- **Best for:** <5 workers, <50 agents

**Option B: Redis Shared Cache (Scalable)**
```typescript
// Replace Map with Redis client
private phraseCache: RedisCache;

async _getFromPhraseCache(text: string): Promise<AudioFrame[]> {
  const key = this._getPhraseCacheKey(text);
  const cached = await redis.get(key);
  if (cached) {
    return JSON.parse(cached);
  }
  return undefined;
}
```
- ‚úÖ Shared across all workers
- ‚úÖ High hit rate (80-90%)
- ‚ùå Network latency (~2-5ms per lookup)
- ‚ùå Additional infrastructure
- **Best for:** >5 workers, >50 agents

**Option C: Hybrid (Best of Both)**
- L1: Local Map cache (5ms lookup)
- L2: Redis fallback (2-5ms lookup)
- Write-through: Cache in both layers
- **Best for:** Production with high traffic

---

### 2. RAG Result Cache (Knowledge Search)

**Location:** `src/services/voice-knowledge/service.ts`

**Purpose:** Cache RAG (Retrieval-Augmented Generation) search results to avoid repeated Convex queries and embeddings.

#### Implementation Details
```typescript
// Singleton shared cache (GLOBAL)
const resultCache = new LRUCache<any>(500, 300); // 500 items, 5 min TTL

export function getResultCache(): LRUCache<any> {
  return resultCache;
}

// Used by VoiceKnowledgeService
private resultCache: LRUCache<VoiceSearchResponse> = getResultCache();

// Cache key format
const cacheKey = `rag:${this.agentId}:${query.toLowerCase().trim()}:${limit}`;
```

#### Lifecycle
- **Created:** Once per Node process (singleton)
- **Shared:** Across ALL agents in the same process
- **Populated:** On first knowledge search
- **Invalidated:** 
  - TTL-based (5 minutes)
  - LRU eviction when size > 500
  - Manual: `invalidateAllCaches()` after KB updates

#### Local vs Production Behavior

**Local:**
```
Agent A query "knee pain" ‚Üí Cache miss ‚Üí Convex query (200ms) ‚Üí Cached
Agent A query "knee pain" ‚Üí Cache HIT (1-2ms)
Agent B query "knee pain" ‚Üí Cache HIT (1-2ms) [shared!]
```

**Production (100 Workers):**
```
Worker 1: Query "knee pain" ‚Üí Cache miss ‚Üí 200ms
Worker 2: Query "knee pain" ‚Üí Cache miss ‚Üí 200ms [duplicate!]
Worker 3: Query "knee pain" ‚Üí Cache miss ‚Üí 200ms [duplicate!]

Result: 100 identical Convex queries in first 5 minutes
```

#### Concurrency Characteristics

**‚úÖ Thread-Safe:** YES
- LRU operations are atomic
- `await` ensures sequential access
- No race conditions

**‚ùå Process-Shared:** NO
- Singleton per process, NOT per cluster
- Each worker has separate cache

**‚ö†Ô∏è Production Concerns:**

1. **Duplicate Queries Across Workers**
   - Same question on different workers = cache miss
   - Wastes Convex API quota
   - Increases latency for users

2. **Memory Multiplication**
   - 500 items √ó 10 workers = 5000 total cache entries
   - Each entry: ~5KB (search results)
   - **Total: ~25MB per worker, ~250MB cluster-wide**

3. **Cache Stampede**
   - 100 concurrent users asking same question
   - All hit same worker at cold start
   - All miss cache simultaneously
   - 100 parallel Convex queries

#### Production Recommendations

**Current State: üü° ACCEPTABLE for <10 workers**

**Production Fixes:**

1. **Short-term: Sticky Sessions**
```nginx
# nginx.conf
upstream livekit_agents {
  ip_hash;  # Same IP ‚Üí same worker
  server worker1:3000;
  server worker2:3000;
}
```
- ‚úÖ Improves cache hit rate to ~60-70%
- ‚úÖ No code changes
- ‚ùå Uneven load distribution

2. **Long-term: Redis Shared Cache**
```typescript
import Redis from 'ioredis';
const redis = new Redis(process.env.REDIS_URL);

async get(key: string): Promise<T | undefined> {
  const cached = await redis.get(key);
  return cached ? JSON.parse(cached) : undefined;
}

async set(key: string, value: T): Promise<void> {
  await redis.setex(key, this.ttlSeconds, JSON.stringify(value));
}
```
- ‚úÖ Shared across all workers
- ‚úÖ Hit rate: 85-95%
- ‚úÖ Reduces Convex queries by 90%
- ‚ùå Adds Redis dependency

3. **Alternative: Increase TTL**
```typescript
// Current: 5 minutes
const resultCache = new LRUCache<any>(500, 300);

// Production: 30 minutes
const resultCache = new LRUCache<any>(1000, 1800);
```
- ‚úÖ Simple change
- ‚úÖ Better hit rate per worker
- ‚ùå Stale data risk (KB updates take 30min to propagate)

---

### 3. Agent Config Cache

**Location:** `src/services/agent-config.ts`

**Purpose:** Cache agent configuration from Convex to avoid repeated database queries.

#### Implementation Details
```typescript
// Per-service instance (one per worker)
private configCache = new Map<string, { config: AgentConfigData; timestamp: number }>();
private configCacheTtl = 60000; // 1 minute

async loadAgentConfig(agentId: string): Promise<AgentConfigData | null> {
  const cached = this.configCache.get(agentId);
  if (cached && Date.now() - cached.timestamp < this.configCacheTtl) {
    return cached.config; // ~1ms
  }
  
  // Fetch from Convex (~100ms)
  const agent = await this.convexQuery('agents:get', { id: agentId });
  
  // Cache for 1 minute
  this.configCache.set(agentId, { config, timestamp: Date.now() });
  return config;
}
```

#### Lifecycle
- **Created:** Once per `AgentConfigService` instance
- **Scope:** All agents handled by this service instance
- **TTL:** 1 minute (short to catch config updates quickly)
- **Eviction:** TTL-based only (no size limit)

#### Local vs Production Behavior

**Local:**
```
Call 1 (Agent A) ‚Üí Config fetch ‚Üí 100ms ‚Üí Cached
Call 2 (Agent A) ‚Üí Cache hit ‚Üí 1ms [within 1 minute]
Call 3 (Agent B) ‚Üí Config fetch ‚Üí 100ms ‚Üí Cached
```

**Production (100 workers):**
```
Worker 1: Agent A ‚Üí Cache miss ‚Üí Convex query
Worker 2: Agent A ‚Üí Cache miss ‚Üí Convex query [duplicate!]
Worker 3: Agent A ‚Üí Cache miss ‚Üí Convex query [duplicate!]

Cache hit rate: ~10% on cold start, ~90% after warm-up
```

#### Concurrency Characteristics

**‚úÖ Thread-Safe:** YES
- Map operations are atomic
- TTL check + fetch is serialized by event loop

**‚ùå Process-Shared:** NO
- Each worker has own cache

**‚ö†Ô∏è Production Concerns:**

1. **Cold Start Thundering Herd**
   - 100 workers start simultaneously
   - All fetch agent configs at once
   - Convex query spike

2. **Memory Growth**
   - No size limit on cache
   - 1000 agents √ó 10 workers = 10,000 cached configs
   - Each config: ~10KB
   - **Total: ~100MB cluster-wide**

3. **Stale Config Window**
   - Config updated in DB
   - Takes up to 1 minute to propagate to all workers
   - Different workers serve different config versions

#### Production Recommendations

**Current State: üü¢ GOOD - No changes needed**

The 1-minute TTL is appropriate because:
- ‚úÖ Short enough for config updates (acceptable delay)
- ‚úÖ Long enough for good hit rate
- ‚úÖ Minimal memory footprint
- ‚úÖ Self-limiting (TTL eviction)

**Optional Enhancement:**
```typescript
// Add size limit with LRU eviction
private configCache = new LRUCache<AgentConfigData>(1000, 60);
```

---

### 4. Agent Prompt Cache

**Location:** `src/services/agent-config.ts`

**Purpose:** Cache pre-built full system prompts (large text) to avoid repeated DB queries and template rendering.

#### Implementation Details
```typescript
// Key includes updatedAt for automatic invalidation
private promptCache = new Map<string, { prompt: string; timestamp: number }>();
private promptCacheTtl = 600000; // 10 minutes

async getCachedFullPrompt(agentId: string): Promise<{ prompt: string | null }> {
  // Load agent to get updatedAt timestamp
  const agent = await this.convexQuery('agents:get', { id: agentId });
  
  // Cache key with timestamp for auto-invalidation
  const cacheKey = `${agentId}:${agent.updatedAt}`;
  
  // Check cache
  const cached = this.promptCache.get(cacheKey);
  if (cached && Date.now() - cached.timestamp < this.promptCacheTtl) {
    return { prompt: cached.prompt, source: 'ram' }; // ~1-2ms
  }
  
  // Use pre-built prompt from DB
  if (agent.fullPrompt) {
    this.promptCache.set(cacheKey, { prompt: agent.fullPrompt, timestamp: Date.now() });
    return { prompt: agent.fullPrompt, source: 'db' }; // ~100ms
  }
}
```

#### Lifecycle
- **Created:** Per `AgentConfigService` instance
- **Cache Key:** `${agentId}:${updatedAt}` (timestamp-based)
- **TTL:** 10 minutes
- **Auto-Invalidation:** Key changes when agent updated

#### Unique Feature: Timestamp-Based Keys

**Smart Invalidation:**
```
Agent A updated at 2026-01-19T10:00:00Z
Cache key: "agent123:2026-01-19T10:00:00Z"

Agent A updated at 2026-01-19T10:05:00Z (prompt changed)
Cache key: "agent123:2026-01-19T10:05:00Z" [NEW KEY!]

Old cache entry naturally expires after 10 minutes
```

This is **superior** to traditional TTL-only caching because:
- ‚úÖ Instant updates (new key = cache miss)
- ‚úÖ No manual invalidation needed
- ‚úÖ Old entries expire gracefully

#### Local vs Production Behavior

**Local:**
```
Call 1 ‚Üí Cache miss ‚Üí DB query (100ms) ‚Üí Cached with timestamp
Call 2 ‚Üí Cache hit (1-2ms)
[Agent updated]
Call 3 ‚Üí New timestamp ‚Üí Cache miss ‚Üí DB query (100ms)
```

**Production:**
```
Worker 1: agent123:v1 ‚Üí Cached
Worker 2: agent123:v1 ‚Üí Cached [duplicate]

[Admin updates agent prompt]

Worker 1: agent123:v2 ‚Üí Cache miss ‚Üí Fresh prompt
Worker 2: agent123:v2 ‚Üí Cache miss ‚Üí Fresh prompt

No stale prompts served! All workers get new version immediately.
```

#### Concurrency Characteristics

**‚úÖ Thread-Safe:** YES
**‚ùå Process-Shared:** NO
**‚úÖ Auto-Invalidation:** YES (timestamp-based keys)

**‚ö†Ô∏è Production Concerns:**

1. **Memory Growth Without Cleanup**
   ```
   Agent updated 100 times = 100 cache entries
   Each entry: ~50KB (large prompt)
   Total: ~5MB per agent
   ```
   
   **Issue:** Old keys never deleted, only expire after 10 min

2. **Cold Start Behavior**
   - Every worker fetches all prompts on first call
   - 1000 agents √ó 10 workers = 10,000 queries on cold start

#### Production Recommendations

**üü° NEEDS IMPROVEMENT**

**Fix: Add Active Cleanup**
```typescript
// Periodic cleanup of expired entries
setInterval(() => {
  const now = Date.now();
  for (const [key, value] of this.promptCache.entries()) {
    if (now - value.timestamp > this.promptCacheTtl) {
      this.promptCache.delete(key);
    }
  }
}, 60000); // Clean every minute
```

**Alternative: Use LRU Cache**
```typescript
private promptCache = new LRUCache<string>(500, 600); // 500 prompts, 10min TTL

// LRU automatically evicts oldest entries
```

---

### 5. WebSocket Connection Pool

**Location:** `src/plugins/sarvam_tts.ts`

**Purpose:** Reuse WebSocket connections to Sarvam API to avoid connection establishment overhead.

#### Implementation Details
```typescript
// Official LiveKit ConnectionPool (thread-safe)
private wsPool: ConnectionPool<WebSocket>;

constructor() {
  this.wsPool = new ConnectionPool<WebSocket>({
    connectCb: async (timeout: number) => this._createWebSocket(timeout),
    closeCb: async (ws: WebSocket) => ws.close(),
    maxSessionDuration: 3600_000, // 1 hour
    connectTimeout: 10_000, // 10 seconds
  });
}

// Pre-warm connections
prewarm(count: number = 2): void {
  this.wsPool.prewarm(count);
}

// Get/return connections
async getPooledConnection(): Promise<WebSocket> {
  return this.wsPool.get(); // Reuses or creates
}

returnToPool(ws: WebSocket): void {
  this.wsPool.put(ws); // Returns for reuse
}
```

#### Lifecycle
- **Created:** Once per TTS instance (one per agent session)
- **Pre-warmed:** 2 connections at agent startup
- **Max Duration:** 1 hour (then replaced)
- **Cleanup:** Automatic on TTS instance destruction

#### Connection Reuse Flow
```
Session 1 (Agent starts):
‚îî‚îÄ Prewarm ‚Üí Create WS1, WS2
‚îî‚îÄ TTS Segment 1 ‚Üí Use WS1
‚îî‚îÄ TTS Segment 2 ‚Üí Use WS2 (parallel!)
‚îî‚îÄ Segment 1 done ‚Üí Return WS1 to pool
‚îî‚îÄ TTS Segment 3 ‚Üí Reuse WS1 ‚úÖ (no new connection!)

Session 2 (Same agent, different user):
‚îî‚îÄ TTS Segment 1 ‚Üí Get WS1 from pool (instant!)
```

#### Local vs Production Behavior

**Local:**
```
Agent 1 (User A) ‚Üí TTS Instance ‚Üí Pool (WS1, WS2)
‚îî‚îÄ 10 TTS requests ‚Üí Reuse WS1, WS2

Agent 1 (User B) ‚Üí NEW TTS Instance ‚Üí NEW Pool (WS3, WS4)
```

**Production:**
```
100 concurrent sessions:
- 100 TTS instances
- 200 WebSocket connections (2 per instance)
- All reused for duration of session (5-10 min)
- Replaced every 1 hour

vs No pooling:
- 1000+ connections (10 TTS calls per session)
- Connection overhead: 50-200ms each
```

#### Concurrency Characteristics

**‚úÖ Thread-Safe:** YES (LiveKit ConnectionPool is designed for concurrency)
**‚úÖ Connection Reuse:** YES
**‚úÖ Auto-Cleanup:** YES (1 hour max age)
**‚ùå Cross-Session Sharing:** NO (pool per TTS instance)

**‚ö†Ô∏è Production Concerns:**

1. **Connection Limits**
   - Sarvam API may have per-IP connection limits
   - 100 agents √ó 2 connections = 200 concurrent WS
   - **Risk:** Hitting Sarvam rate limits

2. **Memory Per Connection**
   - Each WebSocket: ~100KB overhead
   - 200 connections: ~20MB
   - **Acceptable** for production

3. **Cold Start Latency**
   - First 2 TTS requests: 0ms (pre-warmed)
   - Request 3+: 50-200ms (create new connection)
   - Solution: Increase prewarm count

#### Production Recommendations

**üü¢ GOOD - Working as designed**

**Optional Tuning:**
```typescript
// High traffic: Pre-warm more connections
plugins.tts.prewarm(5); // Instead of 2

// Very high traffic: Global connection pool (advanced)
const globalWsPool = new ConnectionPool<WebSocket>({
  // Shared across all TTS instances
  // Reduces total connection count
});
```

---

## üöÄ Production Deployment Checklist

### Immediate Actions (Required)

- [ ] **Add size limit to Agent Prompt Cache**
  ```typescript
  private promptCache = new LRUCache<string>(500, 600);
  ```

- [ ] **Add periodic cleanup for expired cache entries**
  ```typescript
  setInterval(() => this.cleanupExpiredCaches(), 60000);
  ```

- [ ] **Configure sticky sessions (nginx/load balancer)**
  ```nginx
  upstream agents {
    ip_hash;
    server worker1:3000;
    server worker2:3000;
  }
  ```

- [ ] **Monitor cache hit rates**
  ```typescript
  logger.info('Cache stats', {
    phraseCache: tts.getPhraseCacheStats(),
    ragCache: ragCache.getStats(),
  });
  ```

### Short-term Improvements (1-2 weeks)

- [ ] **Implement Redis shared cache for RAG results**
  - Priority: HIGH (biggest performance gain)
  - Impact: 90% reduction in Convex queries
  - Complexity: Medium

- [ ] **Add cache warming on worker startup**
  ```typescript
  async function warmupCaches() {
    // Pre-load top 10 agents
    // Pre-cache common queries
  }
  ```

- [ ] **Implement cache metrics dashboard**
  - Track hit rates
  - Monitor memory usage
  - Alert on cache evictions

### Long-term Optimizations (1-2 months)

- [ ] **Migrate to Redis for all caches**
  - Unified caching layer
  - Consistent behavior across workers
  - Built-in monitoring

- [ ] **Implement cache pre-warming service**
  - Background job warms caches before traffic spike
  - Reduces cold start latency

- [ ] **Add cache invalidation webhooks**
  - Convex ‚Üí Webhook ‚Üí Invalidate specific cache entries
  - Instant updates across all workers

---

## üìà Performance Impact Summary

| Cache Layer | Cache Hit | Cache Miss | Hit Rate (Local) | Hit Rate (Production) |
|-------------|-----------|------------|------------------|----------------------|
| TTS Phrase | 5ms | 1000ms | 90% | 20% (round-robin) |
| RAG Result | 1-2ms | 200ms | 80% | 15% (round-robin) |
| Agent Config | 1ms | 100ms | 95% | 85% |
| Agent Prompt | 1-2ms | 100ms | 95% | 85% |
| WS Connection | 0ms | 50-200ms | 98% | 98% |

**Total Latency Savings (per request):**
- Local: ~1000ms saved
- Production (current): ~300ms saved
- Production (with Redis): ~1000ms saved

---

## üîê Security Considerations

### Cache Poisoning

**Risk:** Malicious user caches bad data that affects others

**Mitigations:**
1. **Isolation:** TTS Phrase Cache is per-agent (no cross-contamination)
2. **TTL:** All caches expire (1 min to 1 hour)
3. **Validation:** Input sanitization before caching
4. **Keys:** MD5 hash includes agent context

### Memory Exhaustion

**Risk:** Unbounded cache growth causes OOM

**Mitigations:**
1. **Size Limits:** TTS (100 items), RAG (500 items)
2. **LRU Eviction:** Oldest items removed first
3. **TTL:** Auto-cleanup after expiration
4. **Monitoring:** Alert on high memory usage

### Stale Data

**Risk:** Cached data becomes outdated

**Mitigations:**
1. **Short TTL:** Config (1 min), RAG (5 min)
2. **Timestamp Keys:** Prompt cache auto-invalidates
3. **Manual Invalidation:** `invalidateAllCaches()` API

---

## üéØ Recommendations Priority

### P0 - Critical (Before Production)
1. ‚úÖ Add size limits to all caches
2. ‚úÖ Implement periodic cleanup
3. ‚úÖ Configure sticky sessions

### P1 - High (Week 1)
1. Redis shared cache for RAG results
2. Cache metrics dashboard
3. Worker pre-warming

### P2 - Medium (Month 1)
1. Redis for all caches
2. Cache invalidation webhooks
3. Advanced monitoring

### P3 - Nice to Have
1. Distributed cache warming
2. Cache analytics
3. A/B testing cache strategies

---

## üìù Conclusion

**Current State:** üü° Production-Ready with Caveats

The caching architecture is well-designed and works excellently in single-worker environments (local development). However, the lack of shared caching across workers creates challenges in production:

**Strengths:**
- ‚úÖ Multiple layers for different use cases
- ‚úÖ Smart invalidation (timestamp-based keys)
- ‚úÖ Memory-efficient per-layer
- ‚úÖ Thread-safe (Node.js single-threaded)

**Weaknesses:**
- ‚ùå No cross-worker cache sharing
- ‚ùå Low hit rates with round-robin load balancing
- ‚ö†Ô∏è Memory multiplication across workers
- ‚ö†Ô∏è Duplicate queries on cold start

**Recommended Action:**
Implement Redis shared cache for RAG results (P1) and add sticky sessions (P0) for immediate production deployment. This will improve hit rates from 15-20% to 80-90% and reduce Convex API load by 90%.

---

**Document Version:** 1.0  
**Last Updated:** January 19, 2026  
**Next Review:** After Redis implementation
